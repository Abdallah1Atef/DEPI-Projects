{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: The Python Scraper's Guild\n",
    "\n",
    "Welcome, apprentice! You've learned the ancient arts of Python's Object-Oriented Programming and the subtle craft of Web Scraping. Now, it's time to combine your skills and prove your worth by building a flexible, reusable, and professional scraping framework from the ground up.\n",
    "\n",
    "Your mission is to architect a series of Python classes that encapsulate scraping logic, making your code clean, scalable, and maintainable. Each part of this project builds upon the last, culminating in a powerful, polymorphic scraping tool.\n",
    "\n",
    "**Your final creation will be a testament to your mastery of both OOP and Web Scraping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Your Toolkit\n",
    "\n",
    "Run the following cell to import all the necessary tools for your journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e8476284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your toolkit is ready!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "print(\"Your toolkit is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3374e90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Guild's Charter - The Scraper Interface\n",
    "\n",
    "Every great guild needs a set of rules and a common blueprint. In OOP, this is called an **Interface** or an **Abstract Base Class (ABC)**. It defines a contract that all members (our scraper classes) must follow, ensuring consistency and predictability.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Define an abstract base class named `Scraper`. This class will serve as the foundation for all future scrapers in our project, enforcing a standard workflow and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "713b94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Scraper ABC here\n",
    "class Scraper(ABC):\n",
    "    def __init__(self, base_url, author_name=None):\n",
    "        self.base_url = base_url\n",
    "        self.author_name = author_name\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fetch_page(self, url):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse_content(self, content):\n",
    "        pass\n",
    "\n",
    "    def scrape(self):\n",
    "        content = self.fetch_page(self.base_url)\n",
    "        if content:\n",
    "            return self.parse_content(content)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaaf823",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The First Contract - The Fake Jobs Board\n",
    "\n",
    "With the guild's charter established, it's time for your first assignment: developing a scraper for the job postings on the \"Real Python Fake Jobs\" board.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Architect a complete solution for scraping and representing job data. This involves creating a specialized data container (`Job`) and a scraper class (`FakeJobsScraper`) that implements the `Scraper` interface. Your final code should be able to fetch all job listings from the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5bcd62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Job class and FakeJobsScraper class here\n",
    "class Job:\n",
    "    def __init__(self, title, company, location, link, description=None):\n",
    "        self.title = title\n",
    "        self.company = company\n",
    "        self.location = location\n",
    "        self.link = link\n",
    "        self.description = description\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"title\": self.title,\n",
    "            \"company\": self.company,\n",
    "            \"location\": self.location,\n",
    "            \"link\": self.link,\n",
    "            \"description\": self.description\n",
    "        }\n",
    "\n",
    "class FakeJobsScraper(Scraper):\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching page: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_content(self, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        jobs = []\n",
    "\n",
    "        job_cards = soup.select(\"div.card-content\")\n",
    "        for card in job_cards:\n",
    "            title = card.select_one('h2.title').text.strip()\n",
    "            company = card.select_one('h3.company').text.strip()\n",
    "            location = card.select_one('p.location').text.strip()\n",
    "            link = urljoin(self.base_url, card.find(\"a\")[\"href\"])\n",
    "\n",
    "            jobs.append(Job(title, company, location, link))\n",
    "\n",
    "        return jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef8dae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Deep Intel - Acquiring Job Descriptions\n",
    "\n",
    "A list of job titles is useful, but a true guild master gathers all available intelligence. Your next task is to enhance your `FakeJobsScraper` to follow the links for each job and extract the full description.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Refactor your existing `FakeJobsScraper` and `Job` classes to support the inclusion of detailed job descriptions. This will require you to manage multiple requests and add new methods while keeping your code clean and encapsulated. Remember to be a responsible scraper and add a small delay between your requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3399f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the enhanced classes here\n",
    "class FakeJobsScraper(FakeJobsScraper):\n",
    "    def parse_content(self, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        jobs = []\n",
    "\n",
    "        job_cards = soup.select(\"div.card-content\")\n",
    "        for card in job_cards:\n",
    "            title = card.select_one('h2.title').text.strip()\n",
    "            company = card.select_one('h3.company').text.strip()\n",
    "            location = card.select_one('p.location').text.strip()\n",
    "            link = urljoin(self.base_url, card.find(\"a\")[\"href\"])\n",
    "\n",
    "            description = self.fetch_job_description(link)\n",
    "            jobs.append(Job(title, company, location, link, description))\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    def fetch_job_description(self, link):\n",
    "        html = self.fetch_page(link)\n",
    "        if not html:\n",
    "            return \"no description available\"\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        desc = soup.select_one(\"div.description\")\n",
    "        return desc.text.strip() if desc else \"no description found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01874d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: A New Territory - The World of Quotes\n",
    "\n",
    "Your framework is proving its worth on one site, but its true value lies in its flexibility. Now, you must prove it can be adapted to a completely different domain: scraping literary quotes from `quotes.toscrape.com`.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Design and implement a new scraper, `QuotesScraper`, and a corresponding `Quote` data class. This scraper must handle a new website structure and a key feature: **pagination**. Your solution should demonstrate the polymorphic power of your `Scraper` interface by scraping all quotes from all pages of the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8e15d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Quote class and QuotesScraper class here\n",
    "class Quote:\n",
    "    def __init__(self, text, author):\n",
    "        self.text = text\n",
    "        self.author = author\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"text\": self.text,\n",
    "            \"author\": self.author\n",
    "        }\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class QuotesScraper(Scraper):\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to fetch page: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_content(self, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        quotes = []\n",
    "\n",
    "        while soup:\n",
    "            quote_divs = soup.select(\"div.quote\")\n",
    "            for quote in quote_divs:\n",
    "                text_tag = quote.find(\"span\", class_=\"text\")\n",
    "                author_tag = quote.find(\"small\", class_=\"author\")\n",
    "                if text_tag and author_tag:\n",
    "                    quote_text = text_tag.text.strip()\n",
    "                    author = author_tag.text.strip()\n",
    "                    \n",
    "                    if self.author_name:\n",
    "                        if author.lower() == self.author_name.lower():\n",
    "                            quotes.append(Quote(quote_text, author))\n",
    "                    else:\n",
    "                        quotes.append(Quote(quote_text, author))\n",
    "                else:\n",
    "                    logging.warning(\"Missing quote text or author.\")\n",
    "\n",
    "            next_btn = soup.select_one(\"li.next > a\")\n",
    "            if next_btn:\n",
    "                next_href = next_btn.get(\"href\")\n",
    "                next_url = urljoin(self.base_url, next_href)\n",
    "                html = self.fetch_page(next_url)\n",
    "                if not html:\n",
    "                    break\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return quotes\n",
    "\n",
    "    @classmethod\n",
    "    def from_author_search(cls, author_name):\n",
    "        base = \"https://quotes.toscrape.com/\"\n",
    "        search_url = urljoin(base, \"search.aspx\")\n",
    "        \n",
    "        resp = requests.get(search_url)\n",
    "        if resp.status_code != 200:\n",
    "            logging.warning(\"Could not load search page\")\n",
    "            return cls(base)\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        a = soup.find(\"a\", string=author_name)\n",
    "        if not a:\n",
    "            logging.warning(f\"Author '{author_name}' not found.\")\n",
    "            return cls(base)\n",
    "\n",
    "        author_url = urljoin(base, a[\"href\"])\n",
    "        return cls(author_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf8135",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: The Codebreaker - Advanced CSS Selectors\n",
    "\n",
    "The `find()` and `find_all()` methods are powerful, but the guild's most elite members are masters of CSS selectors. The `.select()` method in BeautifulSoup allows for more complex and precise element selection.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Enhance your `FakeJobsScraper`. Instead of using multiple `find()` calls to get the title, company, and location, refactor your `parse_content` method to use a single `.select()` call to get all job cards. Then, within your loop, use more specific CSS selectors to extract the required information from each card. This will test your ability to read HTML and construct precise selectors (e.g., selecting elements by attribute, or selecting child elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e17eb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the refactored FakeJobsScraper using .select() here\n",
    "class FakeJobsScraper(FakeJobsScraper):\n",
    "    def parse_content(self, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        jobs = []\n",
    "\n",
    "        job_cards = soup.select(\"div.card-content\")\n",
    "        for card in job_cards:\n",
    "            title = card.select_one('h2.title').get_text(strip=True)\n",
    "            company = card.select_one('h3.company').get_text(strip=True)\n",
    "            location = card.select_one('p.location').get_text(strip=True)\n",
    "            link = urljoin(self.base_url, card.select_one('a.card-footer-item')['href'])\n",
    "            description = self.fetch_job_description(link)\n",
    "            jobs.append(Job(title, company, location, link, description))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        return jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c8438",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Genealogist - Navigating the HTML Tree\n",
    "\n",
    "Sometimes, the data you need isn't in a direct child element. A true master can navigate the entire family tree of an HTML element—finding its parents, siblings, and children.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "On the `quotes.toscrape.com` site, each author's name is a sibling to the `<span>` containing the quote text. Refactor your `QuotesScraper` to demonstrate tree navigation. After finding the quote's text element (`<span class=\"text\">`), use a navigation method like `.find_next_sibling()` to locate the `<small>` tag containing the author's name. This is a more robust method than relying on a fixed structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "14df5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the refactored QuotesScraper using tree navigation here\n",
    "class NavigatingQuotesScraper(QuotesScraper):\n",
    "    def parse_content(self, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        quotes = []\n",
    "\n",
    "        while soup:\n",
    "            quote_divs = soup.select(\"div.quote\")\n",
    "            for quote in quote_divs:\n",
    "                text_tag = quote.find(\"span\", class_=\"text\")\n",
    "                author_tag = quote.find(\"small\", class_=\"author\")\n",
    "                if text_tag and author_tag:\n",
    "                    quotes.append(Quote(text_tag.text.strip(), author_tag.text.strip()))\n",
    "                else:\n",
    "                    logging.warning(\"Missing quote text or author.\")\n",
    "\n",
    "            next_btn = soup.select_one(\"li.next > a\")\n",
    "            if next_btn:\n",
    "                next_href = next_btn.get(\"href\")\n",
    "                next_url = urljoin(self.base_url, next_href)\n",
    "                html = self.fetch_page(next_url)\n",
    "                if not html:\n",
    "                    break\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return quotes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fe180",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: The Scribe's Duty - Storing Your Findings\n",
    "\n",
    "Gathering data is only half the battle. A guild's knowledge is worthless if not properly recorded. You will now create a system for persisting your scraped data to structured files.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Create a new `DataStorage` class responsible for saving data. This task emphasizes the **Single Responsibility Principle**. This class should be flexible enough to save data in different formats.\n",
    "\n",
    "Implement two methods: `save_as_csv(filename, data_objects)` and `save_as_json(filename, data_objects)`. These methods should be ableto handle lists of your `Job` or `Quote` objects and persist them to the specified file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "89e94fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the DataStorage class here\n",
    "class DataStorage:\n",
    "    def save_as_csv(self, filename, data_objects):\n",
    "        if not data_objects:\n",
    "            logging.warning(f\"no data to save to {filename}\")\n",
    "            return\n",
    "\n",
    "        keys = data_objects[0].to_dict().keys()\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            for obj in data_objects:\n",
    "                writer.writerow(obj.to_dict())\n",
    "        logging.info(f\"saved data to {filename}\")\n",
    "\n",
    "    def save_as_json(self, filename, data_objects):\n",
    "        with open(filename, mode='w', encoding='utf-8') as file:\n",
    "            json.dump([obj.to_dict() for obj in data_objects], file, indent=2, ensure_ascii=False)\n",
    "        logging.info(f\"saved data to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116c29f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The Modern Contract - Interacting with APIs\n",
    "\n",
    "Not all web data is hidden in HTML. Modern applications often provide data through an **Application Programming Interface (API)**, usually in JSON format. Your framework must be able to handle this.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Create a new scraper, `JsonPlaceholderScraper`, that inherits from your `Scraper` interface but interacts with a JSON API instead of parsing HTML. You will use the `https://jsonplaceholder.typicode.com/posts` endpoint.\n",
    "\n",
    "Your `fetch_page` method will still use `requests`, but your `parse_content` method will now use `json.loads()` to parse the response text. Create a `Post` data class to hold the `userId`, `id`, `title`, and `body` of each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b4b6cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Post class and JsonPlaceholderScraper class here\n",
    "class Post:\n",
    "    def __init__(self, userId, id, title, body):\n",
    "        self.userId = userId\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"userId\": self.userId,\n",
    "            \"id\": self.id,\n",
    "            \"title\": self.title,\n",
    "            \"body\": self.body\n",
    "        }\n",
    "\n",
    "class JsonPlaceholderScraper(Scraper):\n",
    "    def fetch_page(self, url):\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "\n",
    "    def parse_content(self, content):\n",
    "        json_data = json.loads(content)\n",
    "        return [Post(item['userId'], item['id'], item['title'], item['body']) for item in json_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb990f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: The Watchman's Log - Error Handling & Logging\n",
    "\n",
    "A professional framework must be resilient. Network errors, missing HTML elements, and unexpected changes can break a simple script. A master's tool anticipates and handles these failures gracefully.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Integrate robust error handling and logging into your `Scraper` framework. \n",
    "1.  Modify your base `Scraper`'s `fetch_page` method (or implement it in the children) to include a `try...except` block for `requests.exceptions.RequestException`. If an error occurs, it should log the error and return `None`.\n",
    "2.  In your `parse_content` methods, check if the content you received is `None` before proceeding.\n",
    "3.  Use Python's `logging` module. Configure a basic logger at the beginning of your notebook. Instead of using `print()` for status updates (e.g., \"Scraping page 2...\"), use `logging.info()`. For errors, use `logging.error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "03b1636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for adding logging and error handling here\n",
    "class SafeJsonPlaceholderScraper(JsonPlaceholderScraper):\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"[JsonPlaceholderScraper] failed to fetch page: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_content(self, content):\n",
    "        if not content:\n",
    "            logging.warning(\"[JsonPlaceholderScraper] no content to parse.\")\n",
    "            return []\n",
    "        try:\n",
    "            json_data = json.loads(content)\n",
    "            return [Post(item['userId'], item['id'], item['title'], item['body']) for item in json_data]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"[JsonPlaceholderScraper] JSON decode error: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b0a42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: The Grand Assembly - The Polymorphic Hub\n",
    "\n",
    "This is the final test that proves the power of your object-oriented architecture. You will create a single, elegant loop that can run *any* of your scrapers and use your storage class to save the results.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Instantiate all of your scraper classes and a `DataStorage` object. Create a loop that iterates through your scrapers, runs each one, and then uses the `DataStorage` object to save the results of each scrape to a uniquely named file (e.g., `fake_jobs.csv`, `quotes.json`). This will showcase the power of polymorphism and single responsibility in your completed framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cf371618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the final polymorphic loop and data storage here\n",
    "scrapers = [\n",
    "    NavigatingQuotesScraper(\"https://quotes.toscrape.com/\"),\n",
    "    JsonPlaceholderScraper(\"https://jsonplaceholder.typicode.com/posts\")\n",
    "]\n",
    "storage = DataStorage()\n",
    "for scraper in scrapers:    \n",
    "    data = scraper.scrape()\n",
    "    name = scraper.__class__.__name__.replace(\"Scraper\", \"\").lower()\n",
    "\n",
    "    if name == \"jsonplaceholder\":\n",
    "        storage.save_as_json(f\"{name}.json\", data)\n",
    "    elif name == \"quotes\":\n",
    "        storage.save_as_json(f\"{name}.json\", data)\n",
    "    elif name == \"fakejobs\":\n",
    "        storage.save_as_csv(f\"{name}.csv\", data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master's Challenge \n",
    "\n",
    "If you've completed the guild's main trials, here is an extra challenge to achieve the rank of Master Scraper.\n",
    "\n",
    "### Your Goal:\n",
    "\n",
    "Refactor your `QuotesScraper` to include a `@classmethod` factory. This factory, named `from_author_search`, will allow a user to create a pre-configured scraper instance for a specific author's quotes page just by providing the author's name. This demonstrates a more advanced object creation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigatingQuotesScraper(QuotesScraper):\n",
    "    @classmethod\n",
    "    def from_author_search(cls, author_name):\n",
    "        return cls(\"https://quotes.toscrape.com/page/1/\", author_name=author_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a6b21658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein'}\n",
      "{'text': \"“If you can't explain it to a six year old, you don't understand it yourself.”\", 'author': 'Albert Einstein'}\n",
      "{'text': '“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“Logic will get you from A to Z; imagination will get you everywhere.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“Any fool can know. The point is to understand.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“Life is like riding a bicycle. To keep your balance, you must keep moving.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”', 'author': 'Albert Einstein'}\n",
      "{'text': '“Anyone who has never made a mistake has never tried anything new.”', 'author': 'Albert Einstein'}\n"
     ]
    }
   ],
   "source": [
    "author_scraper = NavigatingQuotesScraper.from_author_search(\"Albert Einstein\")\n",
    "quotes = author_scraper.scrape()\n",
    "for quote in quotes:\n",
    "    print(quote.to_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
